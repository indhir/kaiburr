{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "general code  for cf in cnn,content based , hybrid ."
      ],
      "metadata": {
        "id": "V68qoVU_JFKl"
      },
      "id": "V68qoVU_JFKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VkgWrvWjKdPu"
      },
      "id": "VkgWrvWjKdPu"
    },
    {
      "cell_type": "code",
      "source": [
        "using rows700  data  got output for movieid-661 in content and collaborative and hybrid"
      ],
      "metadata": {
        "id": "4kwuYblKKdkF"
      },
      "id": "4kwuYblKKdkF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import hashlib\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from os.path import isfile, isdir\n",
        "from urllib.request import urlretrieve\n",
        "from tensorflow.python.ops import math_ops\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "3U6mmS-4kav0"
      },
      "id": "3U6mmS-4kav0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "ZxqSkGoykazu"
      },
      "id": "ZxqSkGoykazu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _unzip(save_path, _, database_name, data_path):\n",
        "    \"\"\"Unzip wrapper with the same interface as _ungzip\n",
        "    Args:\n",
        "        save_path: The path of the gzip files\n",
        "        database_name: Name of database\n",
        "        data_path: Path to extract to\n",
        "        _: HACK - Used to have to same interface as _ungzip\n",
        "    \"\"\"\n",
        "    print(f'Extracting { database_name }...')\n",
        "    with zipfile.ZipFile(save_path) as zf:\n",
        "        zf.extractall(data_path)\n",
        "\n",
        "def download_extract(database_name, data_path):\n",
        "    \"\"\"Download and extract database\n",
        "    Args:\n",
        "        database_name: Database name\n",
        "        data_path: Path to extract to\n",
        "    \"\"\"\n",
        "    DATASET_NAME = 'ml-1m'\n",
        "    if database_name == DATASET_NAME:\n",
        "        url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
        "        hash_code = 'c4d9eecfca2ab87c1945afe126590906'\n",
        "        extract_path = os.path.join(data_path, DATASET_NAME)\n",
        "        save_path = os.path.join(data_path, f'{ DATASET_NAME }.zip')\n",
        "        extract_fn = _unzip\n",
        "        \n",
        "    if os.path.exists(extract_path):\n",
        "        print(f'Found { database_name } Data')\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc=f'Downloading { database_name }') as pbar:\n",
        "            urlretrieve(url, save_path, pbar.hook)\n",
        "\n",
        "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
        "        f'{ save_path } file is corrupted.  Remove the file and try again.'\n",
        "\n",
        "    os.makedirs(extract_path)\n",
        "    try:\n",
        "        extract_fn(save_path, extract_path, database_name, data_path)\n",
        "    except Exception as err:\n",
        "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
        "        raise err\n",
        "\n",
        "    print('Done downloading and extracing')\n",
        "    \n",
        "class DLProgress(tqdm):\n",
        "    \"\"\"Handle Progress Bar while Downloading\"\"\"\n",
        "    last_block = 0\n",
        "\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        \"\"\"\n",
        "        A hook function that will be called once on establishment of the network connection and\n",
        "        once after each block read thereafter.\n",
        "        Args:\n",
        "            block_num: A count of blocks transferred so far\n",
        "            block_size: Block size in bytes\n",
        "            total_size: The total size of the file. This may be -1 on older FTP servers which do not return\n",
        "                            a file size in response to a retrieval request.\n",
        "        \"\"\"\n",
        "        self.total = total_size\n",
        "        self.update((block_num - self.last_block) * block_size)\n",
        "        self.last_block = block_num"
      ],
      "metadata": {
        "id": "fEN13mRbjntm"
      },
      "id": "fEN13mRbjntm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './'\n",
        "download_extract('ml-1m', data_dir)"
      ],
      "metadata": {
        "id": "MeTn2xNDjnvn"
      },
      "id": "MeTn2xNDjnvn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
        "users = pd.read_csv('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
        "users.head(700)\n",
        "# print(users.head)\n",
        "# print('-'*20)\n",
        "# age_map = {val:idx for idx, val in enumerate(set(users['Age']))}\n",
        "# print(age_map)\n",
        "# users['Age'] = users['Age'].map(age_map)"
      ],
      "metadata": {
        "id": "2UKYug8ojnzK"
      },
      "id": "2UKYug8ojnzK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_title = ['MovieID', 'Title', 'Genres']\n",
        "movies = pd.read_csv('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python',encoding=\"cp1252\")\n",
        "movies.head(700)"
      ],
      "metadata": {
        "id": "U4qD5yB2jkPC"
      },
      "id": "U4qD5yB2jkPC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
        "ratings = pd.read_csv('ratings.csv', sep='::', header=None, names=ratings_title, engine = 'python')\n",
        "ratings.head(700)"
      ],
      "metadata": {
        "id": "n3U3Q5U_KH58"
      },
      "id": "n3U3Q5U_KH58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = ratings.merge(users,on=['UserID'])\n",
        "Z=merged_data.head(700)\n",
        "print(Z)"
      ],
      "metadata": {
        "id": "_sdhJARzKRXP"
      },
      "id": "_sdhJARzKRXP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = Z.merge(movies,on=['MovieID'])\n",
        "v=merged.head(700)\n",
        "print(v)"
      ],
      "metadata": {
        "id": "qd3hln7kKlMW"
      },
      "id": "qd3hln7kKlMW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data.timestamps.head(700)"
      ],
      "metadata": {
        "id": "C93K541cKtoZ"
      },
      "id": "C93K541cKtoZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_params(params):\n",
        "    \"\"\"Save parameters to file\"\"\"\n",
        "    pickle.dump(params, open('params.pkl', 'wb'))\n",
        "\n",
        "\n",
        "def load_params():\n",
        "    \"\"\"Load parameters from file\"\"\"\n",
        "    return pickle.load(open('params.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "R3spNtaEKwKA"
      },
      "id": "R3spNtaEKwKA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split data into weekdays and weekend"
      ],
      "metadata": {
        "id": "FU2kHQ6UUB3n"
      },
      "id": "FU2kHQ6UUB3n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func(i):\n",
        "     if(i%7 == 0  or i%7 == 6):\n",
        "         return \"weekend\"\n",
        "     else:\n",
        "         return \"weekdays\""
      ],
      "metadata": {
        "id": "_PsAkQ2eKwMa"
      },
      "id": "_PsAkQ2eKwMa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v[\"week\"]=v['timestamps'].apply(lambda x: func(x))"
      ],
      "metadata": {
        "id": "TedVXjxxKwSV"
      },
      "id": "TedVXjxxKwSV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "id": "XZ6aMUPmMl4L"
      },
      "id": "XZ6aMUPmMl4L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(v.week)"
      ],
      "metadata": {
        "id": "4wdOF2zfMnWt"
      },
      "id": "4wdOF2zfMnWt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v.groupby('week')"
      ],
      "metadata": {
        "id": "2MtYLZ07MnZM"
      },
      "id": "2MtYLZ07MnZM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "id": "M2J4hpS4MncQ"
      },
      "id": "M2J4hpS4MncQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=v.groupby('week')"
      ],
      "metadata": {
        "id": "RMrPtA4zbe3V"
      },
      "id": "RMrPtA4zbe3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "pZ3tl1KUbe8M"
      },
      "id": "pZ3tl1KUbe8M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "PXJUubv_eYtD"
      },
      "id": "PXJUubv_eYtD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g=data.get_group('weekdays')\n",
        "g"
      ],
      "metadata": {
        "id": "8jFDeLc25A5Z"
      },
      "id": "8jFDeLc25A5Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "   \n",
        "# Define a dictionary containing ICC rankings\n",
        "\n",
        "   \n",
        "# Convert the dictionary into DataFrame\n",
        "week = pd.DataFrame(g)\n",
        "   \n",
        "# Before renaming the columns\n",
        "print(week)\n",
        "   \n",
        "week.rename(columns = {'UserID':'UID', 'MovieID':'MID','Zip-code':'Zipwd','Title':'TitleWd','Genres':'GenresWd','week':'weekWd',\n",
        "                              'Rating':'RW','timestamp':'timestampWd','Gender':'GenderWd','Age':'Agewd','OccupationID':'OccupationIDWd',}, inplace = True)\n",
        "   \n",
        "# After renaming the columns\n",
        "print(week.columns)"
      ],
      "metadata": {
        "id": "EDuwtvCrrsig"
      },
      "id": "EDuwtvCrrsig",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weeekend alone for q"
      ],
      "metadata": {
        "id": "smAbYfHobnp_"
      },
      "id": "smAbYfHobnp_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g"
      ],
      "metadata": {
        "id": "Jij5Ho0bfmTx"
      },
      "id": "Jij5Ho0bfmTx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekendays input['UIDW', 'MIDW', 'RW', 'timestamps', 'GenderW', 'Agew', 'JobID',\n",
        "       'TitleW', 'GenresW', 'weekW'],\n",
        "      dtype='object')"
      ],
      "metadata": {
        "id": "Hs_XArPzePFT"
      },
      "id": "Hs_XArPzePFT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"Load dataset from file and then preprocessing it\"\"\"\n",
        "    # Load users' data\n",
        "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
        "    users = pd.read_csv('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
        "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
        "    users_origin = users.values\n",
        "    # Preprocess users' gender and age attributes\n",
        "    gender_map = {'F':0, 'M':1}\n",
        "    users['Gender'] = users['Gender'].map(gender_map)\n",
        "    age_map = {val:idx for idx, val in enumerate(set(users['Age']))}\n",
        "    users['Age'] = users['Age'].map(age_map)\n",
        "    \n",
        "    # Load movies' data\n",
        "    movies_title = ['MovieID', 'Title', 'Genres']\n",
        "    movies = pd.read_csv('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python',encoding=\"cp1252\")\n",
        "    movies_origin = movies.values\n",
        "    # Remove year from title\n",
        "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
        "    title_map = {title:pattern.match(title).group(1) for title in set(movies['Title'])}\n",
        "    movies['Title'] = movies['Title'].map(title_map)\n",
        "    # Change movies' genre to dict with numbers\n",
        "    genres_set = set()\n",
        "    for val in movies['Genres'].str.split('|'):\n",
        "        genres_set.update(val)\n",
        "    genres_set.add('<PAD>')\n",
        "    genres2int = {val:idx for idx, val in enumerate(genres_set)}\n",
        "    genres_map = {val:[genres2int[row] for row in val.split('|')] for val in set(movies['Genres'])}\n",
        "    for key in genres_map:\n",
        "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
        "            genres_map[key].insert(len(genres_map[key]) + cnt, genres2int['<PAD>'])\n",
        "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
        "\n",
        "    # Change movies' title to dict with numbers\n",
        "    title_set = set()\n",
        "    for val in movies['Title'].str.split():\n",
        "        title_set.update(val)\n",
        "    title_set.add('<PAD>')\n",
        "    title2int = {val:idx for idx, val in enumerate(title_set)}\n",
        "    title_count = 15\n",
        "    title_map = {val:[title2int[row] for row in val.split()] for val in set(movies['Title'])}\n",
        "    for key in title_map:\n",
        "        for cnt in range(title_count - len(title_map[key])):\n",
        "            title_map[key].insert(len(title_map[key]) + cnt, title2int['<PAD>'])\n",
        "    movies['Title'] = movies['Title'].map(title_map)\n",
        "\n",
        "    # Load ratings' data\n",
        "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
        "    ratings = pd.read_csv('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
        "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
        "\n",
        "    # Merge\n",
        "    data = pd.merge(pd.merge(ratings, users), movies)\n",
        "    \n",
        "    # Split data into two parts\n",
        "    target_fields = ['ratings']\n",
        "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
        "    features = features_pd.values\n",
        "    targets_values = targets_pd.values\n",
        "    \n",
        "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_origin, users_origin"
      ],
      "metadata": {
        "id": "qXtgaUlz0ExQ"
      },
      "id": "qXtgaUlz0ExQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_origin, users_origin = load_data()"
      ],
      "metadata": {
        "id": "-OtqlUXC0VvR"
      },
      "id": "-OtqlUXC0VvR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.head()"
      ],
      "metadata": {
        "id": "xR5zg3By0K6l"
      },
      "id": "xR5zg3By0K6l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies.head()"
      ],
      "metadata": {
        "id": "h7Wvvx2A1CGw"
      },
      "id": "h7Wvvx2A1CGw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimension of matrix\n",
        "embed_dim = 32\n",
        "# Number of users' ids\n",
        "uid_max = max(features.take(0,1)) + 1 # 6040\n",
        "# Number of genders\n",
        "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
        "# Number of ages\n",
        "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
        "# Number of jobs\n",
        "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
        "\n",
        "# Number of movies\n",
        "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
        "# Number of genres\n",
        "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
        "# Number of words of movies' names\n",
        "movie_title_max = len(title_set) # 5216\n",
        "\n",
        "combiner = \"sum\"\n",
        "\n",
        "# Length of movies' name\n",
        "sentences_size = title_count # = 15\n",
        "\n",
        "window_sizes = {2, 3, 4, 5}\n",
        "filter_num = 8\n",
        "movieid2idx = {val[0]:idx for idx, val in enumerate(movies.values)}"
      ],
      "metadata": {
        "id": "uPop98xR1JbM"
      },
      "id": "uPop98xR1JbM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Epochs\n",
        "num_epochs = 5\n",
        "# Batch Size\n",
        "batch_size = 256\n",
        "\n",
        "dropout_keep = 0.5\n",
        "# Learning Rate\n",
        "learning_rate = 0.0001\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 20\n",
        "\n",
        "save_dir = './save'"
      ],
      "metadata": {
        "id": "z3Ov9h2x1NUD"
      },
      "id": "z3Ov9h2x1NUD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inputs():\n",
        "    uid = tf.keras.layers.Input(shape=(1,), dtype='int32', name='UserID')  \n",
        "    user_gender = tf.keras.layers.Input(shape=(1,), dtype='int32', name='Gender')  \n",
        "    user_age = tf.keras.layers.Input(shape=(1,), dtype='int32', name='Age') \n",
        "    user_job = tf.keras.layers.Input(shape=(1,), dtype='int32', name='')\n",
        "\n",
        "    movie_id = tf.keras.layers.Input(shape=(1,), dtype='int32', name=\t'MovieID') \n",
        "    movie_categories = tf.keras.layers.Input(shape=(18,), dtype='int32', name='Genres') \n",
        "    movie_titles = tf.keras.layers.Input(shape=(15,), dtype='int32', name='Title') \n",
        "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles"
      ],
      "metadata": {
        "id": "5v4_5NB61Q7i"
      },
      "id": "5v4_5NB61Q7i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
        "    uid_embed_layer = tf.keras.layers.Embedding(uid_max, embed_dim, input_length=1, name='uid_embed_layer')(uid)\n",
        "    gender_embed_layer = tf.keras.layers.Embedding(gender_max, embed_dim // 2, input_length=1, name='gender_embed_layer')(user_gender)\n",
        "    age_embed_layer = tf.keras.layers.Embedding(age_max, embed_dim // 2, input_length=1, name='age_embed_layer')(user_age)\n",
        "    job_embed_layer = tf.keras.layers.Embedding(job_max, embed_dim // 2, input_length=1, name='job_embed_layer')(user_job)\n",
        "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
      ],
      "metadata": {
        "id": "08PD1_mV1Q9j"
      },
      "id": "08PD1_mV1Q9j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
        "    # First FCL\n",
        "    uid_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"uid_fc_layer\", activation='relu')(uid_embed_layer)\n",
        "    gender_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"gender_fc_layer\", activation='relu')(gender_embed_layer)\n",
        "    age_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"age_fc_layer\", activation='relu')(age_embed_layer)\n",
        "    job_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"job_fc_layer\", activation='relu')(job_embed_layer)\n",
        "\n",
        "    # Second FCL\n",
        "    user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
        "    user_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(user_combine_layer)  #(?, 1, 200)\n",
        "\n",
        "    user_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_combine_layer)\n",
        "    return user_combine_layer, user_combine_layer_flat"
      ],
      "metadata": {
        "id": "IFTdpoGq1Wmr"
      },
      "id": "IFTdpoGq1Wmr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_id_embed_layer(movie_id):\n",
        "    movie_id_embed_layer = tf.keras.layers.Embedding(movie_id_max, embed_dim, input_length=1, name='movie_id_embed_layer')(movie_id)\n",
        "    return movie_id_embed_layer"
      ],
      "metadata": {
        "id": "AbbPZiO61Wo-"
      },
      "id": "AbbPZiO61Wo-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_categories_layers(movie_categories):\n",
        "    movie_categories_embed_layer = tf.keras.layers.Embedding(movie_categories_max, embed_dim, input_length=18, name='movie_categories_embed_layer')(movie_categories)\n",
        "    movie_categories_embed_layer = tf.keras.layers.Lambda(lambda layer: tf.reduce_sum(layer, axis=1, keepdims=True))(movie_categories_embed_layer)\n",
        "    return movie_categories_embed_layer"
      ],
      "metadata": {
        "id": "btySnsql1Wr2"
      },
      "id": "btySnsql1Wr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_cnn_layer(movie_titles):\n",
        "    movie_title_embed_layer = tf.keras.layers.Embedding(movie_title_max, embed_dim, input_length=15, name='movie_title_embed_layer')(movie_titles)\n",
        "    sp = movie_title_embed_layer.shape\n",
        "    movie_title_embed_layer_expand = tf.keras.layers.Reshape([sp[1], sp[2], 1])(movie_title_embed_layer)\n",
        "    pool_layer_lst = []\n",
        "    for window_size in window_sizes:\n",
        "        conv_layer = tf.keras.layers.Conv2D(filter_num, (window_size, embed_dim), 1, activation='relu')(movie_title_embed_layer_expand)\n",
        "        maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(sentences_size - window_size + 1 ,1), strides=1)(conv_layer)\n",
        "        pool_layer_lst.append(maxpool_layer)\n",
        "    pool_layer = tf.keras.layers.concatenate(pool_layer_lst, 3, name =\"pool_layer\")  \n",
        "    max_num = len(window_sizes) * filter_num\n",
        "    pool_layer_flat = tf.keras.layers.Reshape([1, max_num], name = \"pool_layer_flat\")(pool_layer)\n",
        "\n",
        "    dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
        "    return pool_layer_flat, dropout_layer"
      ],
      "metadata": {
        "id": "y9fPUGsI1Wuo"
      },
      "id": "y9fPUGsI1Wuo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
        "    # First FCL 64\n",
        "    movie_id_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_id_fc_layer\", activation='relu')(movie_id_embed_layer)\n",
        "    movie_categories_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_categories_fc_layer\", activation='relu')(movie_categories_embed_layer)\n",
        "\n",
        "    # Second FCL 200\n",
        "    movie_combine_layer = tf.keras.layers.concatenate([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  \n",
        "    movie_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(movie_combine_layer)\n",
        "\n",
        "    movie_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"movie_combine_layer_flat\")(movie_combine_layer)\n",
        "    return movie_combine_layer, movie_combine_layer_flat"
      ],
      "metadata": {
        "id": "hzBp8Lmu1RA2"
      },
      "id": "hzBp8Lmu1RA2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers"
      ],
      "metadata": {
        "id": "DR9ZFmqv1jci"
      },
      "id": "DR9ZFmqv1jci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.ops import summary_ops_v2"
      ],
      "metadata": {
        "id": "WyfNc2db1je3"
      },
      "id": "WyfNc2db1je3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = \"./models\""
      ],
      "metadata": {
        "id": "TqbpfoOH1jiC"
      },
      "id": "TqbpfoOH1jiC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mrs_network():\n",
        "    def __init__(self, batch_size=256):\n",
        "        self.batch_size = batch_size\n",
        "        self.best_loss = 9999\n",
        "        self.losses = {'train': [], 'test': []}\n",
        "        \n",
        "        # User's input\n",
        "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = get_inputs()\n",
        "        \n",
        "        # User's embedding layers\n",
        "        uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, \n",
        "                                                                                                   user_gender,\n",
        "                                                                                                   user_age,\n",
        "                                                                                                   user_job)\n",
        "        # User's feature\n",
        "        user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, \n",
        "                                                                             gender_embed_layer, \n",
        "                                                                             age_embed_layer, \n",
        "                                                                             job_embed_layer)\n",
        "        \n",
        "        # Movie's id embedding layer\n",
        "        movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
        "        # Movie's genre embedding layer\n",
        "        movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
        "        # Movie's name layer\n",
        "        pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
        "        # Movie's feature\n",
        "        movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer,\n",
        "                                                                                movie_categories_embed_layer,\n",
        "                                                                                dropout_layer)\n",
        "        # Combine user's feature and movie's feature to get rating prediction\n",
        "        inference = tf.keras.layers.Lambda(lambda layer: \n",
        "            tf.reduce_sum(layer[0] * layer[1], axis=1), name=\"inference\")((user_combine_layer_flat, movie_combine_layer_flat))\n",
        "        inference = tf.keras.layers.Lambda(lambda layer: tf.expand_dims(layer, axis=1))(inference)\n",
        "        \n",
        "        self.model = tf.keras.Model(\n",
        "            inputs=[uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles],\n",
        "            outputs=[inference])\n",
        "\n",
        "        self.model.summary()\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "        # MSE Loss\n",
        "        self.ComputeLoss = tf.keras.losses.MeanSquaredError()\n",
        "        self.ComputeMetrics = tf.keras.metrics.MeanAbsoluteError()\n",
        "        \n",
        "        if tf.io.gfile.exists(MODEL_DIR):\n",
        "            pass\n",
        "        else:\n",
        "            tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "        train_dir = os.path.join(MODEL_DIR, 'summaries', 'train')\n",
        "        test_dir = os.path.join(MODEL_DIR, 'summaries', 'eval')\n",
        "\n",
        "        checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
        "        self.checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "        self.checkpoint = tf.train.Checkpoint(model=self.model, optimizer=self.optimizer)\n",
        "\n",
        "        # Restore variables on creation if a checkpoint exists.\n",
        "        self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "    def compute_loss(self, labels, logits):\n",
        "        return tf.reduce_mean(tf.keras.losses.mse(labels, logits))\n",
        "\n",
        "    def compute_metrics(self, labels, logits):\n",
        "        return tf.keras.metrics.mae(labels, logits)\n",
        "        \n",
        "    @tf.function\n",
        "    def train_step(self, x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = self.model([x[0],\n",
        "                                 x[1],\n",
        "                                 x[2],\n",
        "                                 x[3],\n",
        "                                 x[4],\n",
        "                                 x[5],\n",
        "                                 x[6]], training=True)\n",
        "            loss = self.ComputeLoss(y, logits)\n",
        "            self.ComputeMetrics(y, logits)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss, logits\n",
        "    \n",
        "    def training(self, features, targets_values, epochs=5, log_freq=50):\n",
        "        for epoch_i in range(epochs):\n",
        "            # separate the dataset into training and testing\n",
        "            train_X, test_X, train_y, test_y = train_test_split(features,\n",
        "                                                                targets_values,\n",
        "                                                                test_size=0.2,\n",
        "                                                                random_state=0)\n",
        "\n",
        "            train_batches = get_batches(train_X, train_y, self.batch_size)\n",
        "            batch_num = (len(train_X) // self.batch_size)\n",
        "            train_start = time.time()\n",
        "            \n",
        "            if True:\n",
        "                start = time.time()\n",
        "                avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "\n",
        "                for batch_i in range(batch_num):\n",
        "                    x, y = next(train_batches)\n",
        "                    categories = np.zeros([self.batch_size, 18])\n",
        "                    for i in range(self.batch_size):\n",
        "                        categories[i] = x.take(6, 1)[i]\n",
        "\n",
        "                    titles = np.zeros([self.batch_size, sentences_size])\n",
        "                    for i in range(self.batch_size):\n",
        "                        titles[i] = x.take(5, 1)[i]\n",
        "\n",
        "                    loss, logits = self.train_step([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                                    categories.astype(np.float32),\n",
        "                                                    titles.astype(np.float32)],\n",
        "                                                   np.reshape(y, [self.batch_size, 1]).astype(np.float32))\n",
        "                    avg_loss(loss)\n",
        "                    self.losses['train'].append(loss)\n",
        "\n",
        "                    if tf.equal(self.optimizer.iterations % log_freq, 0):\n",
        "                        rate = log_freq / (time.time() - start)\n",
        "                        print('Step #{}\\tEpoch {:>3} Batch {:>4}/{}   Loss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
        "                            self.optimizer.iterations.numpy(),\n",
        "                            epoch_i,\n",
        "                            batch_i,\n",
        "                            batch_num,\n",
        "                            loss, (self.ComputeMetrics.result()), rate))\n",
        "                        avg_loss.reset_states()\n",
        "                        self.ComputeMetrics.reset_states()\n",
        "                        start = time.time()\n",
        "\n",
        "            train_end = time.time()\n",
        "            print('\\nTrain time for epoch #{} ({} total steps): {}'.format(epoch_i + 1, \n",
        "                                                                           self.optimizer.iterations.numpy(),\n",
        "                                                                           train_end - train_start))\n",
        "            self.testing((test_X, test_y), self.optimizer.iterations)\n",
        "        self.export_path = os.path.join(MODEL_DIR, 'export')\n",
        "        tf.saved_model.save(self.model, self.export_path)\n",
        "    \n",
        "    def testing(self, test_dataset, step_num):\n",
        "        test_X, test_y = test_dataset\n",
        "        test_batches = get_batches(test_X, test_y, self.batch_size)\n",
        "\n",
        "        avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "\n",
        "        batch_num = (len(test_X) // self.batch_size)\n",
        "        for batch_i in range(batch_num):\n",
        "            x, y = next(test_batches)\n",
        "            categories = np.zeros([self.batch_size, 18])\n",
        "            for i in range(self.batch_size):\n",
        "                categories[i] = x.take(6, 1)[i]\n",
        "\n",
        "            titles = np.zeros([self.batch_size, sentences_size])\n",
        "            for i in range(self.batch_size):\n",
        "                titles[i] = x.take(5, 1)[i]\n",
        "\n",
        "            logits = self.model([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
        "                                 categories.astype(np.float32),\n",
        "                                 titles.astype(np.float32)], training=False)\n",
        "            test_loss = self.ComputeLoss(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
        "            avg_loss(test_loss)\n",
        "            # Save testing loss\n",
        "            self.losses['test'].append(test_loss)\n",
        "            self.ComputeMetrics(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
        "\n",
        "        print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), \n",
        "                                                                 self.ComputeMetrics.result()))\n",
        "\n",
        "        if avg_loss.result() < self.best_loss:\n",
        "            self.best_loss = avg_loss.result()\n",
        "            print(f'best loss = { self.best_loss }')\n",
        "            self.checkpoint.save(self.checkpoint_prefix)\n",
        "\n",
        "    \n",
        "    def forward(self, xs):\n",
        "        predictions = self.model(xs)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "hLXgmRj41uD4"
      },
      "id": "hLXgmRj41uD4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(Xs, ys, batch_size):\n",
        "    for start in range(0, len(Xs), batch_size):\n",
        "        end = min(start + batch_size, len(Xs))\n",
        "        yield Xs[start:end], ys[start:end]"
      ],
      "metadata": {
        "id": "jB67lo3k1yzA"
      },
      "id": "jB67lo3k1yzA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mrs_net = mrs_network()"
      ],
      "metadata": {
        "id": "daWOModB18Z9"
      },
      "id": "daWOModB18Z9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mrs_net.training(features, targets_values, epochs=1)"
      ],
      "metadata": {
        "id": "LfTJsTQE1y1I"
      },
      "id": "LfTJsTQE1y1I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(mrs_net.losses['train'], label='Training loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UBxPA4lE1y4b"
      },
      "id": "UBxPA4lE1y4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(mrs_net.losses['test'], label='Test loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qgrs0CSO1y6a"
      },
      "id": "Qgrs0CSO1y6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rating_movie(mrs_net, user_id_val, movie_id_val):\n",
        "    categories = np.zeros([1, 18])\n",
        "    categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
        "    \n",
        "    titles = np.zeros([1, sentences_size])\n",
        "    titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
        "    \n",
        "    inference_val = mrs_net.model([np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
        "              np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
        "              np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
        "              np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
        "              np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
        "              categories,  \n",
        "              titles])\n",
        "\n",
        "    return (inference_val.numpy())"
      ],
      "metadata": {
        "id": "BoDUfYqN1y9z"
      },
      "id": "BoDUfYqN1y9z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_layer_model = keras.models.Model(inputs=[mrs_net.model.input[0], mrs_net.model.input[1], mrs_net.model.input[2], mrs_net.model.input[3]], \n",
        "                                 outputs=mrs_net.model.get_layer(\"user_combine_layer_flat\").output)\n",
        "users_matrics = []\n",
        "\n",
        "for item in users.values:\n",
        "    user_combine_layer_flat_val = user_layer_model([np.reshape(item.take(0), [1, 1]), \n",
        "                                                    np.reshape(item.take(1), [1, 1]), \n",
        "                                                    np.reshape(item.take(2), [1, 1]), \n",
        "                                                    np.reshape(item.take(3), [1, 1])])  \n",
        "    users_matrics.append(user_combine_layer_flat_val)\n",
        "\n",
        "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "BHC4HVNo1zAi"
      },
      "id": "BHC4HVNo1zAi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_matrics = pickle.load(open('users_matrics.pkl', mode='rb'))\n",
        "print(users_matrics)"
      ],
      "metadata": {
        "id": "SuhdmQkcFEyT"
      },
      "id": "SuhdmQkcFEyT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_layer_model = keras.models.Model(inputs=[mrs_net.model.input[4], mrs_net.model.input[5], mrs_net.model.input[6]], \n",
        "                                 outputs=mrs_net.model.get_layer(\"movie_combine_layer_flat\").output)\n",
        "movie_matrics = []\n",
        "\n",
        "for item in movies.values:\n",
        "    categories = np.zeros([1, 18])\n",
        "    categories[0] = item.take(2)\n",
        "\n",
        "    titles = np.zeros([1, sentences_size])\n",
        "    titles[0] = item.take(1)\n",
        "\n",
        "    movie_combine_layer_flat_val = movie_layer_model([np.reshape(item.take(0), [1, 1]), categories, titles])  \n",
        "    movie_matrics.append(movie_combine_layer_flat_val)\n",
        "\n",
        "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "MWi4WCVMFE0o"
      },
      "id": "MWi4WCVMFE0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_matrics = pickle.load(open('movie_matrics.pkl', mode='rb'))\n",
        "print(movie_matrics)"
      ],
      "metadata": {
        "id": "-vc7JrhnFE30"
      },
      "id": "-vc7JrhnFE30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(movieid2idx)\n",
        "# print(tf.reshape(movie_matrics[movieid2idx[2]], [1, 200]))\n",
        "norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keepdims=True))\n",
        "normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
        "print(tf.transpose(normalized_movie_matrics))"
      ],
      "metadata": {
        "id": "Ex_aynCS1uGZ"
      },
      "id": "Ex_aynCS1uGZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_same_genre_movie(movie_id_val, top_k = 20):\n",
        "   \n",
        "    norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keepdims=True))\n",
        "    normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
        "\n",
        "    # Recommend movies with same genre\n",
        "    probs_embeddings = tf.reshape(movie_matrics[movieid2idx[movie_id_val]], [1, 200])\n",
        "    probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
        "    sim = (probs_similarity.numpy())\n",
        "\n",
        "    print(f\"The movie you're watching is：{ movies_origin[movieid2idx[movie_id_val]] }\")\n",
        "    print(\"Here are recommendations for you：\")\n",
        "    p = np.squeeze(sim)\n",
        "    p[np.argsort(p)[:-top_k]] = 0\n",
        "    p = p / np.sum(p)\n",
        "    results = set()\n",
        "    while len(results) != 5:\n",
        "        c = np.random.choice(3883, 1, p=p)[0]\n",
        "        results.add(c)\n",
        "    for val in (results):\n",
        "        print(val)\n",
        "        print(movies_origin[val])\n",
        "        \n",
        "    return results"
      ],
      "metadata": {
        "id": "geInPu3NFQkl"
      },
      "id": "geInPu3NFQkl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_same_genre_movie(914, 20)"
      ],
      "metadata": {
        "id": "DZy8OgAjWeDN"
      },
      "id": "DZy8OgAjWeDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_same_genre_movie(661, 20)"
      ],
      "metadata": {
        "id": "J1x7u7leGo0S"
      },
      "id": "J1x7u7leGo0S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_same_genre_movie(1197, 20)"
      ],
      "metadata": {
        "id": "QRprnp9Qji34"
      },
      "id": "QRprnp9Qji34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_same_genre_movie(594, 20)"
      ],
      "metadata": {
        "id": "s_olXM2-jp0u"
      },
      "id": "s_olXM2-jp0u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n"
      ],
      "metadata": {
        "id": "o6WTin9kKM_a"
      },
      "id": "o6WTin9kKM_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab=pd.DataFrame(recommend_same_genre_movie(661,20))"
      ],
      "metadata": {
        "id": "e3ahCKryA0Aq"
      },
      "id": "e3ahCKryA0Aq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab[\"collaborative\"]= pd.DataFrame(recommend_same_genre_movie(661,20))"
      ],
      "metadata": {
        "id": "PHLxD_x0IkYD"
      },
      "id": "PHLxD_x0IkYD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
        "    # Recommend your favorite movies\n",
        "    probs_embeddings = tf.reshape(users_matrics[user_id_val-1], [1, 200])\n",
        "    probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
        "    sim = (probs_similarity.numpy())\n",
        "    \n",
        "    print(\"Here are recommendations for you：\")\n",
        "    p = np.squeeze(sim)\n",
        "    p[np.argsort(p)[:-top_k]] = 0\n",
        "    p = p / np.sum(p)\n",
        "    results = set()\n",
        "    while len(results) != 5:\n",
        "        c = np.random.choice(3883, 1, p=p)[0]\n",
        "        results.add(c)\n",
        "    for val in (results):\n",
        "        print(val)\n",
        "        print(movies_origin[val])\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "yrboFaz2FWVr"
      },
      "id": "yrboFaz2FWVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_your_favorite_movie(1, 10)"
      ],
      "metadata": {
        "id": "NBYQIjYpFb85"
      },
      "id": "NBYQIjYpFb85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
        "    probs_movie_embeddings = tf.reshape(movie_matrics[movieid2idx[movie_id_val]], [1, 200])\n",
        "    probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
        "    favorite_user_id = np.argsort(probs_user_favorite_similarity.numpy())[0][-top_k:]\n",
        "    \n",
        "    print(f\"The movie you watching is：{ movies_origin[movieid2idx[movie_id_val]] }\")\n",
        "    print(f\"People who like the movie are：{ users_origin[favorite_user_id - 1] }\")\n",
        "    \n",
        "    probs_users_embeddings = tf.reshape(users_matrics[favorite_user_id-1], [-1, 200])\n",
        "    probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
        "    sim = (probs_similarity.numpy())\n",
        "    p = np.argmax(sim, 1)\n",
        "    print(\"Other movies may like：\")\n",
        "\n",
        "    if len(set(p)) < 5:\n",
        "        results = set(p)\n",
        "    else:\n",
        "        results = set()\n",
        "        while len(results) != 5:\n",
        "            c = p[random.randrange(top_k)]\n",
        "            results.add(c)\n",
        "    for val in (results):\n",
        "        print(val)\n",
        "        print(movies_origin[val])\n",
        "        \n",
        "    return results"
      ],
      "metadata": {
        "id": "5nAkG3nEFcB9"
      },
      "id": "5nAkG3nEFcB9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_matrics = pickle.load(open('users_matrics.pkl', mode='rb'))\n",
        "print(users_matrics)\n",
        "movie_matrics = pickle.load(open('movie_matrics.pkl', mode='rb'))\n",
        "print(movie_matrics)"
      ],
      "metadata": {
        "id": "THpQV_IGFj9V"
      },
      "id": "THpQV_IGFj9V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend_other_favorite_movie(1401, 20)"
      ],
      "metadata": {
        "id": "FyHwHON_Fj_Y"
      },
      "id": "FyHwHON_Fj_Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "+++++++++++++++++++++++++++++++++++++++++++cf end++++++++++++++++++++++++++++++++++++++++++++++++"
      ],
      "metadata": {
        "id": "WDoyPUCMzOy-"
      },
      "id": "WDoyPUCMzOy-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content based "
      ],
      "metadata": {
        "id": "H1Gy_gw6mUuM"
      },
      "id": "H1Gy_gw6mUuM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_title = ['MovieID', 'Title', 'Genres']\n",
        "md = pd.read_csv('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python',encoding=\"cp1252\")\n",
        "md.head()"
      ],
      "metadata": {
        "id": "DpSKod_U5I2W"
      },
      "id": "DpSKod_U5I2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md=v\n",
        "md"
      ],
      "metadata": {
        "id": "YwH0RGkqJ073"
      },
      "id": "YwH0RGkqJ073",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content- movie title as input - to get titlecontent"
      ],
      "metadata": {
        "id": "hTnO3x2c8QJr"
      },
      "id": "hTnO3x2c8QJr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = md['MovieID']"
      ],
      "metadata": {
        "id": "gu9-D0uG8QXg"
      },
      "id": "gu9-D0uG8QXg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['tagline'] = v['week'].fillna('')\n"
      ],
      "metadata": {
        "id": "m1NCukF59ryb"
      },
      "id": "m1NCukF59ryb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n"
      ],
      "metadata": {
        "id": "TmkkNaOlMygk"
      },
      "id": "TmkkNaOlMygk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "tfidf_matrix = tf.fit_transform(smd['tagline'])"
      ],
      "metadata": {
        "id": "HrR4E1AA96jZ"
      },
      "id": "HrR4E1AA96jZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "uTf4xadC9r2k"
      },
      "id": "uTf4xadC9r2k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ],
      "metadata": {
        "id": "j0PiKLR49r7B"
      },
      "id": "j0PiKLR49r7B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim[0]"
      ],
      "metadata": {
        "id": "ht7_l4BH-QPe"
      },
      "id": "ht7_l4BH-QPe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = v.reset_index()\n",
        "titles =  smd['Title']\n",
        "indices =pd.Series(smd.index, index=smd['Title'])"
      ],
      "metadata": {
        "id": "N4zWG3sP-QTc"
      },
      "id": "N4zWG3sP-QTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(title):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:31]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return titles.iloc[movie_indices]"
      ],
      "metadata": {
        "id": "B7Ytw5_9-QXs"
      },
      "id": "B7Ytw5_9-QXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations(\"James and the Giant Peach (1996)\").head(10)"
      ],
      "metadata": {
        "id": "kY2LSz3l-qF9"
      },
      "id": "kY2LSz3l-qF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab[\"titlecontent\"]=pd.DataFrame(get_recommendations(\"James and the Giant Peach (1996)\").head(10))"
      ],
      "metadata": {
        "id": "sTV6Ag31-bUe"
      },
      "id": "sTV6Ag31-bUe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab[\"titlecontent\"]"
      ],
      "metadata": {
        "id": "64Pxs6YZJ40o"
      },
      "id": "64Pxs6YZJ40o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content title wd"
      ],
      "metadata": {
        "id": "avaAM3yiIOi1"
      },
      "id": "avaAM3yiIOi1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab"
      ],
      "metadata": {
        "id": "Typ3JHMO-6H8"
      },
      "id": "Typ3JHMO-6H8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "0sQOA6uMNGr0"
      },
      "id": "0sQOA6uMNGr0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " movie id   as input  for content "
      ],
      "metadata": {
        "id": "aOIcHgrifc6i"
      },
      "id": "aOIcHgrifc6i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = v['MovieID']"
      ],
      "metadata": {
        "id": "x4qYSjWvAqU9"
      },
      "id": "x4qYSjWvAqU9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['tagline'] = v['week'].fillna('')\n"
      ],
      "metadata": {
        "id": "TVfxCKCOAqZp"
      },
      "id": "TVfxCKCOAqZp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "tfidf_matrix = tf.fit_transform(smd['tagline'])"
      ],
      "metadata": {
        "id": "WH-A_wBjAqd8"
      },
      "id": "WH-A_wBjAqd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "9Muf_vCsAqiF"
      },
      "id": "9Muf_vCsAqiF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ],
      "metadata": {
        "id": "fH_iqp3fAqm3"
      },
      "id": "fH_iqp3fAqm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim[0]"
      ],
      "metadata": {
        "id": "tcUW_qjPBH1E"
      },
      "id": "tcUW_qjPBH1E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = v.reset_index()\n",
        "titles =  smd['MovieID']\n",
        "indices =pd.Series(smd.index, index=smd['MovieID'])"
      ],
      "metadata": {
        "id": "hDPOjcz9BH5Q"
      },
      "id": "hDPOjcz9BH5Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(title):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:31]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return titles.iloc[movie_indices]"
      ],
      "metadata": {
        "id": "1qDtpEIwBH9f"
      },
      "id": "1qDtpEIwBH9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations(661).head(10)"
      ],
      "metadata": {
        "id": "1bZ5rWpBBIBX"
      },
      "id": "1bZ5rWpBBIBX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab[\"content\"]=pd.DataFrame(get_recommendations(661).head(10))"
      ],
      "metadata": {
        "id": "qfPMyqTg5FvE"
      },
      "id": "qfPMyqTg5FvE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid for content and collaborative method  recommendation using same movieid in content and collaborative method ,will show content,collaborative and title of that id ."
      ],
      "metadata": {
        "id": "9dQp3ypC9GO7"
      },
      "id": "9dQp3ypC9GO7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab"
      ],
      "metadata": {
        "id": "PgiBnkHO5Nai"
      },
      "id": "PgiBnkHO5Nai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "9YE9Pbz9NBOl"
      },
      "id": "9YE9Pbz9NBOl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                   END"
      ],
      "metadata": {
        "id": "-pbDINz29jhg"
      },
      "id": "-pbDINz29jhg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rX7BqUXL9mXT"
      },
      "id": "rX7BqUXL9mXT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PZ0yI0HU9mZT"
      },
      "id": "PZ0yI0HU9mZT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r1-nw7T39mcl"
      },
      "id": "r1-nw7T39mcl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3z96uO-59me0"
      },
      "id": "3z96uO-59me0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q3a-Zras9mi4"
      },
      "id": "Q3a-Zras9mi4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n1TSqszd9mpE"
      },
      "id": "n1TSqszd9mpE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FG2_5GVH9msA"
      },
      "id": "FG2_5GVH9msA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WhijUcEz9mu3"
      },
      "id": "WhijUcEz9mu3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xg3zLcGp9myC"
      },
      "id": "Xg3zLcGp9myC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = g['MIDW']"
      ],
      "metadata": {
        "id": "DkAT-A3HnvJh"
      },
      "id": "DkAT-A3HnvJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['tagline'] = g['MIDW'].fillna('')\n"
      ],
      "metadata": {
        "id": "JLyz2Nfkn71u"
      },
      "id": "JLyz2Nfkn71u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "tfidf_matrix = tf.fit_transform(smd['tagline'])"
      ],
      "metadata": {
        "id": "YrV1JjeZn75U"
      },
      "id": "YrV1JjeZn75U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "t6pZUBxIn78-"
      },
      "id": "t6pZUBxIn78-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ],
      "metadata": {
        "id": "yRe8CQzsn8Ab"
      },
      "id": "yRe8CQzsn8Ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim[0]"
      ],
      "metadata": {
        "id": "URySuFqZn8Dc"
      },
      "id": "URySuFqZn8Dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = g.reset_index()\n",
        "titles =  smd['MIDW']\n",
        "indices =pd.Series(smd.index, index=smd['MIDW'])"
      ],
      "metadata": {
        "id": "pt3LQEIpn8HD"
      },
      "id": "pt3LQEIpn8HD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(title):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:31]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return titles.iloc[movie_indices]"
      ],
      "metadata": {
        "id": "KiQxfbUhoPW_"
      },
      "id": "KiQxfbUhoPW_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations(1193).head(10)"
      ],
      "metadata": {
        "id": "N9dp_NSgoPZ6"
      },
      "id": "N9dp_NSgoPZ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LK0hdfZBw74J"
      },
      "id": "LK0hdfZBw74J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wCZwthskw79S"
      },
      "id": "wCZwthskw79S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nU_w7VMGu7Sy"
      },
      "id": "nU_w7VMGu7Sy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dgwxdd-du7Wi"
      },
      "id": "Dgwxdd-du7Wi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kkffEKWRu7Z7"
      },
      "id": "kkffEKWRu7Z7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = v['UserID']"
      ],
      "metadata": {
        "id": "1thLWBX9uIfo"
      },
      "id": "1thLWBX9uIfo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['tagline'] = v['UserID'].fillna('')"
      ],
      "metadata": {
        "id": "-iqieLedufCh"
      },
      "id": "-iqieLedufCh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "tfidf_matrix = tf.fit_transform(smd['tagline'])"
      ],
      "metadata": {
        "id": "BzUQq3uuufGg"
      },
      "id": "BzUQq3uuufGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "c_csuTlkuzZl"
      },
      "id": "c_csuTlkuzZl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ],
      "metadata": {
        "id": "H9b3x49tuzdy"
      },
      "id": "H9b3x49tuzdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim[0]"
      ],
      "metadata": {
        "id": "yxRbM71iuzib"
      },
      "id": "yxRbM71iuzib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "id": "0OLdYtNOwsti"
      },
      "id": "0OLdYtNOwsti",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = v.reset_index()\n",
        "titles =  smd['UserID']\n",
        "indices =pd.Series(smd.index, index=smd['UserID'])"
      ],
      "metadata": {
        "id": "Ph-Bzk9iu7Od"
      },
      "id": "Ph-Bzk9iu7Od",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(titles):\n",
        "    idx = indices[titles]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:31]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return titles.iloc[movie_indices]"
      ],
      "metadata": {
        "id": "s8tI-RilufKY"
      },
      "id": "s8tI-RilufKY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations(1).head(10)"
      ],
      "metadata": {
        "id": "d0-tZQQWvc5-"
      },
      "id": "d0-tZQQWvc5-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ofVpeZudvc9u"
      },
      "id": "ofVpeZudvc9u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H_9NR5RVvdBf"
      },
      "id": "H_9NR5RVvdBf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid using mid - weekdays and weekends for content and collaborative methods"
      ],
      "metadata": {
        "id": "WmIXN1j8uInf"
      },
      "id": "WmIXN1j8uInf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "based on movie id in both content and collaborative  hybrid and recommending movies "
      ],
      "metadata": {
        "id": "o9zomn2GvCc0"
      },
      "id": "o9zomn2GvCc0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab[\"collaborative\"]=pd.DataFrame(get_recommendations(594).head(10))"
      ],
      "metadata": {
        "id": "ds7lwtiolpc2"
      },
      "id": "ds7lwtiolpc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ab"
      ],
      "metadata": {
        "id": "ePc4jQBOlvhV"
      },
      "id": "ePc4jQBOlvhV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CGkF1TMxP8y0"
      },
      "id": "CGkF1TMxP8y0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def(MID):\n",
        "  "
      ],
      "metadata": {
        "id": "vU0fXmilP82F"
      },
      "id": "vU0fXmilP82F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the smaller links file again\n",
        "links_df = pd.read_csv('../input/the-movies-dataset/links_small.csv')\n",
        "col=np.array(links_df['tmdbId'], np.int64)\n",
        "links_df['tmdbId']=col\n",
        "\n",
        "# Merge movies_metadata.csv and links_small.csv files\n",
        "links_df = links_df.merge(meta[['title', 'tmdbId']], on='tmdbId').set_index('title')\n",
        "links_index = links_df.set_index('tmdbId') # For label indexing"
      ],
      "metadata": {
        "id": "3zieSc4kuIq3"
      },
      "id": "3zieSc4kuIq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_PT5yF8Z3k6T"
      },
      "id": "_PT5yF8Z3k6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Dqn7dU9MWaN"
      },
      "id": "4Dqn7dU9MWaN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_ow9R2oxSLTS"
      },
      "id": "_ow9R2oxSLTS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pu8BOPovSLWR"
      },
      "id": "pu8BOPovSLWR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SLsR_KffP8GJ"
      },
      "id": "SLsR_KffP8GJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1DkExlSzSLZG"
      },
      "id": "1DkExlSzSLZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hwGoNBdNSLcH"
      },
      "id": "hwGoNBdNSLcH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GAD5qEE8SLeS"
      },
      "id": "GAD5qEE8SLeS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yU4kEm-ScGGs"
      },
      "id": "yU4kEm-ScGGs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AMvy6iIeAF4S"
      },
      "id": "AMvy6iIeAF4S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xfPJI8HVAF7S"
      },
      "id": "xfPJI8HVAF7S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fNPT9YyH72n0"
      },
      "id": "fNPT9YyH72n0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oqVmAlVh7p4-"
      },
      "id": "oqVmAlVh7p4-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UzwtLMAK7klL"
      },
      "id": "UzwtLMAK7klL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bdw5Tuxb7fja"
      },
      "id": "Bdw5Tuxb7fja",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AajyD6aY7aRS"
      },
      "id": "AajyD6aY7aRS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BwMAAQser8sU"
      },
      "id": "BwMAAQser8sU"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VFZ_gDlIbW3x"
      },
      "id": "VFZ_gDlIbW3x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q9wZ8UmoN7El"
      },
      "id": "Q9wZ8UmoN7El",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qeR3SL341jIR"
      },
      "id": "qeR3SL341jIR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WBGMSKL71jMd"
      },
      "id": "WBGMSKL71jMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iwXdkT3q1jQU"
      },
      "id": "iwXdkT3q1jQU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ry-PIQId1jTx"
      },
      "id": "Ry-PIQId1jTx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-ugXLP-o1jXC"
      },
      "id": "-ugXLP-o1jXC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BJg24DTXI-GP"
      },
      "id": "BJg24DTXI-GP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "weekdays_as_input,hybrid_method_for_recommendation_(22)_(5) (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}